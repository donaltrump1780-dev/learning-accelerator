[
  {
    "lessonId": "tokenization",
    "inLessonQuizzes": [
      {
        "afterSection": "concept",
        "title": "Quick Check: Tokenization Basics",
        "questions": [
          {
            "id": "tok-q1",
            "question": "Why do LLMs need tokenization?",
            "options": [
              "Neural networks can only process numbers, not raw text",
              "To make text look prettier",
              "To compress files",
              "To translate languages"
            ],
            "correct": 0,
            "explanation": "Neural networks operate on numbers (tensors), not strings. Tokenization converts text into numeric IDs that the model can process mathematically."
          },
          {
            "id": "tok-q2",
            "question": "What happens when you tokenize 'Hello world'?",
            "options": [
              "It stays as 'Hello world'",
              "It becomes numbers like [15496, 995]",
              "It gets deleted",
              "It becomes binary code"
            ],
            "correct": 1,
            "explanation": "Each token (word/subword) gets mapped to a unique ID from the vocabulary. 'Hello' might be 15496, 'world' might be 995 in GPT's tokenizer."
          },
          {
            "id": "tok-q3",
            "question": "Why not just use one character = one token?",
            "options": [
              "Too many characters exist",
              "Sequences would be way too long and lose meaning",
              "Characters are illegal",
              "It would work perfectly"
            ],
            "correct": 1,
            "explanation": "'Hello world' would be 11 tokens at character level vs 2 at word level. Longer sequences = more computation and harder to capture meaning."
          }
        ]
      }
    ]
  },
  {
    "lessonId": "special-tokens",
    "inLessonQuizzes": [
      {
        "afterSection": "concept",
        "title": "Quick Check: Special Tokens",
        "questions": [
          {
            "id": "spec-q1",
            "question": "What does <|endoftext|> tell the model?",
            "options": [
              "Keep generating",
              "This is where one document/conversation ends",
              "Delete everything",
              "Start over"
            ],
            "correct": 1,
            "explanation": "<|endoftext|> is a boundary marker. During training, it separates different documents. During generation, it can signal when to stop."
          },
          {
            "id": "spec-q2",
            "question": "Why does GPT need <|unk|> (unknown) tokens?",
            "options": [
              "It doesn't",
              "For words it has never seen before",
              "To confuse users",
              "For encryption"
            ],
            "correct": 1,
            "explanation": "If you input a rare word not in the vocabulary, the model needs a way to handle it. <|unk|> represents 'I don't know this word.'"
          }
        ]
      }
    ]
  },
  {
    "lessonId": "byte-pair-encoding",
    "inLessonQuizzes": [
      {
        "afterSection": "concept",
        "title": "Quick Check: BPE",
        "questions": [
          {
            "id": "bpe-q1",
            "question": "What problem does Byte Pair Encoding solve?",
            "options": [
              "How to balance vocabulary size with sequence length",
              "How to encrypt data",
              "How to print text",
              "How to delete files"
            ],
            "correct": 0,
            "explanation": "BPE finds the sweet spot: not too many tokens (like character-level), not too few (like word-level). It merges frequent pairs to build an optimal vocabulary."
          },
          {
            "id": "bpe-q2",
            "question": "If 'un' + 'lock' appear together often, BPE will:",
            "options": [
              "Delete them",
              "Merge them into a single 'unlock' token",
              "Keep them separate forever",
              "Encrypt them"
            ],
            "correct": 1,
            "explanation": "BPE iteratively merges the most frequent pairs. If 'unlock' appears often, it becomes one token instead of two, making sequences shorter."
          }
        ]
      }
    ]
  },
  {
    "lessonId": "data-sampling",
    "inLessonQuizzes": [
      {
        "afterSection": "concept",
        "title": "Quick Check: Data Sampling",
        "questions": [
          {
            "id": "samp-q1",
            "question": "Why use a sliding window for training data?",
            "options": [
              "To see outside",
              "To create overlapping sequences that teach the model context",
              "To save memory",
              "Windows are fast"
            ],
            "correct": 1,
            "explanation": "A sliding window creates many training examples from one document. Each window shifts by a few tokens, so the model sees the same text in different contexts."
          },
          {
            "id": "samp-q2",
            "question": "What is a training 'batch'?",
            "options": [
              "One word",
              "Multiple sequences processed together in parallel",
              "A type of cookie",
              "A single letter"
            ],
            "correct": 1,
            "explanation": "Training in batches (e.g., 32 sequences at once) is much faster than one-at-a-time. GPUs excel at parallel processing."
          }
        ]
      }
    ]
  },
  {
    "lessonId": "embeddings",
    "inLessonQuizzes": [
      {
        "afterSection": "concept",
        "title": "Quick Check: Embeddings",
        "questions": [
          {
            "id": "emb-q1",
            "question": "What is an embedding?",
            "options": [
              "A fancy word for nothing",
              "A vector that represents a token's meaning in high-dimensional space",
              "A type of furniture",
              "A programming language"
            ],
            "correct": 1,
            "explanation": "Embeddings convert tokens (numbers) into dense vectors (e.g., 512 or 1024 dimensions) that capture semantic meaning. Similar words have similar vectors."
          },
          {
            "id": "emb-q2",
            "question": "Why are embeddings better than one-hot encoding?",
            "options": [
              "They're not",
              "They capture relationships between words (king - man + woman â‰ˆ queen)",
              "They use less memory always",
              "They're easier to type"
            ],
            "correct": 1,
            "explanation": "One-hot vectors treat all words as equally different. Embeddings place similar words (like 'king' and 'queen') close together in vector space, capturing meaning."
          },
          {
            "id": "emb-q3",
            "question": "Embedding dimensions (512, 1024, etc.) represent:",
            "options": [
              "How many words exist",
              "Different aspects of meaning the model can learn",
              "File size",
              "Number of GPUs"
            ],
            "correct": 1,
            "explanation": "Each dimension can capture a different aspect: formality, sentiment, topic, tense, etc. More dimensions = more nuanced representations (but more parameters)."
          }
        ]
      }
    ]
  }
]
