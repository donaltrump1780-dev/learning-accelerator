[
  {
    "lessonId": "tokenization",
    "exercises": [
      {
        "id": "tok-ex1",
        "title": "Build a Simple Tokenizer",
        "instructions": "Create a function that splits text into words and punctuation. Test it on the sentence: 'Hello, world! How are you?'",
        "starterCode": "import re\n\ndef simple_tokenizer(text):\n    # TODO: Use regex to find words and punctuation\n    # Hint: r\"\\b\\w+\\b|[.,!?;]\" matches words OR punctuation\n    tokens = []  # Replace this\n    return tokens\n\n# Test\ntext = \"Hello, world! How are you?\"\ntokens = simple_tokenizer(text)\nprint(f\"Tokens: {tokens}\")\nprint(f\"Count: {len(tokens)}\")",
        "solution": "import re\n\ndef simple_tokenizer(text):\n    pattern = r\"\\b\\w+\\b|[.,!?;]\"\n    tokens = re.findall(pattern, text)\n    return tokens\n\ntext = \"Hello, world! How are you?\"\ntokens = simple_tokenizer(text)\nprint(f\"Tokens: {tokens}\")\nprint(f\"Count: {len(tokens)}\")",
        "hints": [
          "Use re.findall() to extract all matches",
          "The pattern \\b\\w+\\b matches whole words",
          "[.,!?;] matches individual punctuation marks"
        ],
        "expectedOutput": "Tokens: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\nCount: 8"
      }
    ]
  },
  {
    "lessonId": "special-tokens",
    "exercises": [
      {
        "id": "spec-ex1",
        "title": "Add Special Tokens",
        "instructions": "Extend the tokenizer to add <|start|> at the beginning and <|end|> at the end of sequences.",
        "starterCode": "def add_special_tokens(tokens):\n    # TODO: Add <|start|> at beginning and <|end|> at end\n    return tokens  # Modify this\n\n# Test\ntokens = ['Hello', 'world']\nresult = add_special_tokens(tokens)\nprint(result)",
        "solution": "def add_special_tokens(tokens):\n    return ['<|start|>'] + tokens + ['<|end|>']\n\ntokens = ['Hello', 'world']\nresult = add_special_tokens(tokens)\nprint(result)",
        "hints": [
          "Use list concatenation: [start] + tokens + [end]",
          "Remember special tokens are strings"
        ],
        "expectedOutput": "['<|start|>', 'Hello', 'world', '<|end|>']"
      }
    ]
  },
  {
    "lessonId": "byte-pair-encoding",
    "exercises": [
      {
        "id": "bpe-ex1",
        "title": "Count Token Pairs",
        "instructions": "Count how many times each pair of consecutive tokens appears. This is the first step of BPE.",
        "starterCode": "from collections import Counter\n\ndef count_pairs(tokens):\n    # TODO: Count consecutive pairs\n    # Example: ['a', 'b', 'a', 'b'] -> {('a','b'): 2}\n    pairs = []\n    return Counter(pairs)\n\n# Test\ntokens = ['un', 'lock', 'un', 'lock', 'un', 'able']\npair_counts = count_pairs(tokens)\nprint(pair_counts)",
        "solution": "from collections import Counter\n\ndef count_pairs(tokens):\n    pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    return Counter(pairs)\n\ntokens = ['un', 'lock', 'un', 'lock', 'un', 'able']\npair_counts = count_pairs(tokens)\nprint(pair_counts)",
        "hints": [
          "Use a list comprehension with range(len(tokens)-1)",
          "Each pair is (tokens[i], tokens[i+1])",
          "Counter from collections counts occurrences automatically"
        ],
        "expectedOutput": "Counter({('un', 'lock'): 2, ('lock', 'un'): 1, ('un', 'able'): 1})"
      }
    ]
  },
  {
    "lessonId": "data-sampling",
    "exercises": [
      {
        "id": "samp-ex1",
        "title": "Create Sliding Windows",
        "instructions": "Generate training windows from a token sequence. Window size = 4, stride = 2.",
        "starterCode": "def create_windows(tokens, window_size=4, stride=2):\n    # TODO: Create overlapping windows\n    windows = []\n    return windows\n\n# Test\ntokens = [1, 2, 3, 4, 5, 6, 7, 8]\nwindows = create_windows(tokens, window_size=4, stride=2)\nfor i, w in enumerate(windows):\n    print(f\"Window {i+1}: {w}\")",
        "solution": "def create_windows(tokens, window_size=4, stride=2):\n    windows = []\n    for i in range(0, len(tokens) - window_size + 1, stride):\n        windows.append(tokens[i:i+window_size])\n    return windows\n\ntokens = [1, 2, 3, 4, 5, 6, 7, 8]\nwindows = create_windows(tokens, window_size=4, stride=2)\nfor i, w in enumerate(windows):\n    print(f\"Window {i+1}: {w}\")",
        "hints": [
          "Use range(0, len(tokens) - window_size + 1, stride)",
          "Extract each window with tokens[i:i+window_size]",
          "Stride controls overlap (stride=1 is max overlap)"
        ],
        "expectedOutput": "Window 1: [1, 2, 3, 4]\nWindow 2: [3, 4, 5, 6]\nWindow 3: [5, 6, 7, 8]"
      }
    ]
  },
  {
    "lessonId": "embeddings",
    "exercises": [
      {
        "id": "emb-ex1",
        "title": "Create Embedding Matrix",
        "instructions": "Build a simple embedding lookup. Vocabulary size = 100, embedding dimension = 8.",
        "starterCode": "import torch\nimport torch.nn as nn\n\n# TODO: Create embedding layer\nvocab_size = 100\nembed_dim = 8\nembedding = None  # Replace with nn.Embedding(...)\n\n# Test with token IDs\ntoken_ids = torch.tensor([5, 10, 15])\nembeddings = embedding(token_ids)\nprint(f\"Shape: {embeddings.shape}\")\nprint(f\"First embedding:\\n{embeddings[0]}\")",
        "solution": "import torch\nimport torch.nn as nn\n\nvocab_size = 100\nembed_dim = 8\nembedding = nn.Embedding(vocab_size, embed_dim)\n\ntoken_ids = torch.tensor([5, 10, 15])\nembeddings = embedding(token_ids)\nprint(f\"Shape: {embeddings.shape}\")  # Should be (3, 8)\nprint(f\"First embedding:\\n{embeddings[0]}\")",
        "hints": [
          "Use nn.Embedding(vocab_size, embed_dim)",
          "Pass token IDs as a tensor to get embeddings",
          "Each token gets a 8-dimensional vector"
        ],
        "expectedOutput": "Shape: torch.Size([3, 8])\nFirst embedding:\ntensor([...])  # Random 8-dimensional vector"
      }
    ]
  }
]
